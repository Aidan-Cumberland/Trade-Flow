{"cells": [{"cell_type": "code", "execution_count": 1, "id": "3b095f71-7db0-4957-9295-24f49e942621", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/05/04 02:37:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# Import PySpark and other necessary libraries\n# Install Java\n#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n\n# Install PySpark\n#!pip install pyspark\n#import os\n#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, count, year, month\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nspark = SparkSession.builder.appName('Trade Flow Prediction').getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "d9f4ce95-f2b6-43bd-aee4-a70f0c7d87b4", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#import pandas as pd\n\nbig_df = spark.read.csv(\"gs://acumberl_data_for_gcp_labs/Trade Flow/Trade_Flow_Big_Dataset.csv\", header=True, inferSchema=True)\nsmall_df = spark.read.csv(\"gs://acumberl_data_for_gcp_labs/Trade Flow/Trade_Flow_Small_Dataset.csv\", header=True, inferSchema=True)\n"}, {"cell_type": "code", "execution_count": 3, "id": "df7e96e2-a205-4061-a8ee-a432ab7d781e", "metadata": {"tags": []}, "outputs": [], "source": "# DECIDE HERE WHETER TO RUN THE BIG OR SMALL DATASET\n\nspark_df = big_df"}, {"cell_type": "code", "execution_count": 4, "id": "bdf0c0f8-a484-43e4-af90-b6e1bd534220", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-----------------+----------------------+----------------+-----------+------------+\n|      Date|Country_of_Origin|Country_of_Destination|Product_Category|Trade_Value|Trade_Volume|\n+----------+-----------------+----------------------+----------------+-----------+------------+\n|2012-05-10|            Japan|                 Italy|        Textiles|    1652.53|       265.0|\n|2020-04-30|            Italy|                Russia|            Food|    3718.66|        81.0|\n|2018-06-20|           Russia|                Canada|       Machinery|    7414.49|      1708.0|\n|2011-04-12|          Germany|                France|       Machinery|    4939.41|      1045.0|\n|2022-02-13|           Canada|                France|        Textiles|    1878.51|       569.0|\n+----------+-----------------+----------------------+----------------+-----------+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 5:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----------------+----------------------+----------------+-----------+------------+\n|Date|Country_of_Origin|Country_of_Destination|Product_Category|Trade_Value|Trade_Volume|\n+----+-----------------+----------------------+----------------+-----------+------------+\n|   0|                0|                     0|               0|          0|           0|\n+----+-----------------+----------------------+----------------+-----------+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Data Exploration and Cleaning\n# Show the first few rows of the dataframe\nspark_df.show(5)\n\n# Checking for missing values in each column\nspark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()\n\n# Removing rows with missing values\nspark_df = spark_df.na.drop()"}, {"cell_type": "code", "execution_count": 5, "id": "24de1a85-09c3-4699-8630-3215718186e5", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Feature Engineering\n# Indexing categorical columns\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(spark_df) for column in ['Country_of_Origin', 'Country_of_Destination', 'Product_Category']]\n\n# Assemble vectors\nassembler = VectorAssembler(inputCols=['Country_of_Origin_index', 'Country_of_Destination_index', 'Product_Category_index', 'Trade_Value'],\n                            outputCol='features')\n\n# Random Forest Model\n\nrf = RandomForestRegressor(featuresCol='features', labelCol='Trade_Volume')\n\n# Set up the pipeline\npipeline = Pipeline(stages=indexers + [assembler, rf])"}, {"cell_type": "code", "execution_count": 6, "id": "669f7a5b-da97-4c29-a50d-2bade7618422", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "WARNING: An illegal reflective access operation has occurred                    \nWARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.5.0.jar) to field java.nio.charset.Charset.name\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\n[Stage 31:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------+------------------+\n|            features|Trade_Volume|        prediction|\n+--------------------+------------+------------------+\n|[5.0,6.0,5.0,6451...|      1218.0|499.53811668271226|\n|[5.0,3.0,5.0,1131...|       488.0|500.68440834565956|\n|[5.0,1.0,1.0,2291...|      1714.0|509.02961986769344|\n|[5.0,7.0,6.0,3983...|       964.0| 506.7589940831146|\n|[7.0,0.0,4.0,1594...|       207.0| 500.4471280953273|\n+--------------------+------------+------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Model Training\n\n# Split the data into training and testing sets\ntrain_data, test_data = spark_df.randomSplit([0.8, 0.2], seed=42)\n\n# Fit the model\nrf_model = pipeline.fit(train_data)\n\n# Make predictions\nrf_predictions = rf_model.transform(test_data)\n\n# Show sample predictions\nrf_predictions.select('features', 'Trade_Volume', 'prediction').show(5)"}, {"cell_type": "code", "execution_count": 7, "id": "9e6cd553-76b1-49be-b3ce-6e2dbb28f4cc", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 32:>                                                         (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Root Mean Squared Error (RMSE) on test data = 498.92875435783685\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Random Forest Model Evaluation\n\nevaluator = RegressionEvaluator(labelCol='Trade_Volume', predictionCol='prediction', metricName='rmse')\nrmse = evaluator.evaluate(rf_predictions)\nprint(f'Root Mean Squared Error (RMSE) on test data = {rmse}')\n"}, {"cell_type": "code", "execution_count": 8, "id": "0adae8f2-10af-4ae3-8cfd-d10b1761c0ac", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/05/04 02:39:18 WARN Instrumentation: [7c022ede] regParam is zero, which might cause numerical instability and overfitting.\n[Stage 35:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------+------------------+\n|            features|Trade_Volume|        prediction|\n+--------------------+------------+------------------+\n|[5.0,6.0,5.0,6451...|      1218.0|499.32118113547574|\n|[5.0,3.0,5.0,1131...|       488.0|499.42589174232256|\n|[5.0,1.0,1.0,2291...|      1714.0| 499.0535805583509|\n|[5.0,7.0,6.0,3983...|       964.0|500.84440735301274|\n|[7.0,0.0,4.0,1594...|       207.0| 499.5986357808394|\n+--------------------+------------+------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Linear Regression Model\n\nfrom pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol='features', labelCol='Trade_Volume')\n\nlr_pipeline = Pipeline(stages=indexers + [assembler, lr])\n\n# Model Training\n# Fit the model\nlr_model = lr_pipeline.fit(train_data)\n\n# Make predictions\nlr_predictions = lr_model.transform(test_data)\n\n# Show sample predictions\nlr_predictions.select('features', 'Trade_Volume', 'prediction').show(5)"}, {"cell_type": "code", "execution_count": 9, "id": "d0a79a03-5249-4ebf-be7c-af33c10baabd", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 36:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Root Mean Squared Error (RMSE) on test data = 498.92875435783685\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Linear Regression Model Evaluation\n\nlr_rmse = evaluator.evaluate(lr_predictions)\nprint(f'Root Mean Squared Error (RMSE) on test data = {rmse}')"}, {"cell_type": "code", "execution_count": 10, "id": "d8ddeac2-b434-457b-b97d-eda0b851b24b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 37:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of rows: 1000000\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "num_rows = spark_df.count()\nprint(f\"Number of rows: {num_rows}\")"}, {"cell_type": "code", "execution_count": null, "id": "ecd21b03-a6c5-4992-ab43-3f759868673d", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 5}